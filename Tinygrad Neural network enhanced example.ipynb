{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9b0aeb-c93c-450e-9155-3b4142b34edf",
   "metadata": {},
   "source": [
    "This code uses the tinygrad library to build and train a variety of simple models on the MNIST dataset, which consists of 28x28 pixel images of handwritten digits (0-9). It's a bit more complicated than the previous one, and it has more components to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279282a2-8b30-450b-9852-4737e8a6d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "from tinygrad.state import get_parameters\n",
    "from tinygrad.tensor import Tensor, Device\n",
    "from tinygrad.nn import optim, BatchNorm2d\n",
    "from extra.training import train, evaluate\n",
    "from datasets import fetch_mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29456a59-94b8-4ddd-9675-4ee36324bcda",
   "metadata": {},
   "source": [
    "In addition to the previously mentioned tinygrad components, it also uses unittest for structuring tests, numpy for numerical computations, get_parameters function to fetch parameters from the model, and BatchNorm2d for batch normalization, a technique that helps neural networks train faster and more stably. The train and evaluate functions are imported from the extra.training module, and the fetch_mnist function is imported from the datasets module to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc5c82-a6b7-479c-b71f-fa583e0dec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = fetch_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96249dd-a9de-4214-91ea-50269223d0e0",
   "metadata": {},
   "source": [
    "This line fetches the MNIST data and divides it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679655d0-b6b7-40f5-adbb-f395ff0af953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBobNet:\n",
    "  def __init__(self):\n",
    "    self.l1 = Tensor.scaled_uniform(784, 128)\n",
    "    self.l2 = Tensor.scaled_uniform(128, 10)\n",
    "\n",
    "  def parameters(self):\n",
    "    return get_parameters(self)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x.dot(self.l1).relu().dot(self.l2).log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd731a2-0997-4592-828f-6d119c310801",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The TinyBobNet is similar to the one in the first code but has a new method, parameters, that uses the get_parameters function to collect the model's parameters, in this case, the tensors l1 and l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523292e-9246-492b-9278-0079c298d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model with a conv layer\n",
    "class TinyConvNet:\n",
    "  def __init__(self, has_batchnorm=False):\n",
    "    # https://keras.io/examples/vision/mnist_convnet/\n",
    "    conv = 3\n",
    "    #inter_chan, out_chan = 32, 64\n",
    "    inter_chan, out_chan = 8, 16   # for speed\n",
    "    self.c1 = Tensor.scaled_uniform(inter_chan,1,conv,conv)\n",
    "    self.c2 = Tensor.scaled_uniform(out_chan,inter_chan,conv,conv)\n",
    "    self.l1 = Tensor.scaled_uniform(out_chan*5*5, 10)\n",
    "    if has_batchnorm:\n",
    "      self.bn1 = BatchNorm2d(inter_chan)\n",
    "      self.bn2 = BatchNorm2d(out_chan)\n",
    "    else:\n",
    "      self.bn1, self.bn2 = lambda x: x, lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0d0ee-1681-4c9f-8df0-f1f010dafdaf",
   "metadata": {},
   "source": [
    "This class, TinyConvNet, represents a simple Convolutional Neural Network, which is particularly useful for image processing tasks.\n",
    "\n",
    "In the __init__ method, we're setting up the layers for this network:\n",
    "\n",
    "conv is the size of the convolution filter, set to 3, which means we'll use 3x3 filters.\n",
    "inter_chan and out_chan represent the number of channels or 'depth' of the output from the first and second convolutional layers, respectively.\n",
    "self.c1 and self.c2 are the first and second convolutional layers. They are initialized with weights from a scaled uniform distribution.\n",
    "self.l1 is a fully connected layer that comes after the convolutional layers, useful for classifying the features extracted by the convolutions.\n",
    "If has_batchnorm is True, we create two batch normalization layers (self.bn1 and self.bn2), which can help improve the speed, performance, and stability of the network. If has_batchnorm is False, self.bn1 and self.bn2 will simply return their input unchanged (they're set as identity functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a62d21-d722-4482-8b5a-61885683c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def parameters(self):\n",
    "    return get_parameters(self)\n",
    "\n",
    "  def forward(self, x:Tensor):\n",
    "    x = x.reshape(shape=(-1, 1, 28, 28)) # hacks\n",
    "    x = self.bn1(x.conv2d(self.c1)).relu().max_pool2d()\n",
    "    x = self.bn2(x.conv2d(self.c2)).relu().max_pool2d()\n",
    "    x = x.reshape(shape=[x.shape[0], -1])\n",
    "    return x.dot(self.l1).log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99b4a3-a87d-4731-8aa0-34436ac8c45c",
   "metadata": {},
   "source": [
    "These methods, parameters and forward, are crucial parts of our TinyConvNet class:\n",
    "\n",
    "The parameters method returns a list of all the learnable parameters of the model, including the weights of the convolution and dense layers, and the parameters of the batch normalization layers (if they exist). It uses the get_parameters function from the tinygrad library, which retrieves these parameters from the instance (self).\n",
    "\n",
    "The forward method is responsible for performing the actual computations of the model on the input data. The input tensor x passes through the following steps:\n",
    "\n",
    "x is reshaped to have a 4D shape, typically corresponding to [Batch size, Channels, Height, Width].\n",
    "The reshaped x is passed through the first convolutional layer c1, followed by the first batch normalization layer bn1 (or identity function if batch normalization is not used), a ReLU activation function, and a max pooling operation.\n",
    "The output from step 2 is passed through the second convolutional layer c2, followed by the second batch normalization layer bn2, another ReLU activation function, and another max pooling operation.\n",
    "The output is then reshaped again, flattened into a 2D tensor [Batch size, Features], preparing it for the fully connected layer.\n",
    "The reshaped tensor is passed through the fully connected layer l1.\n",
    "Finally, a log softmax function is applied, useful for classification tasks as it turns logits into probabilities.\n",
    "The output of the forward method is the model's prediction given the input x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71ba26-d887-4c4e-9f3d-f325addbac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMNIST(unittest.TestCase):\n",
    "  def test_sgd_onestep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=1)\n",
    "    for p in model.parameters(): p.realize()\n",
    "\n",
    "  def test_sgd_threestep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=3)\n",
    "\n",
    "  def test_sgd_sixstep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=6, noloss=True)\n",
    "\n",
    "  def test_adam_onestep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=1)\n",
    "    for p in model.parameters(): p.realize()\n",
    "\n",
    "  def test_adam_threestep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=3)\n",
    "\n",
    "  def test_conv_onestep(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyConvNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, BS=69, steps=1, noloss=True)\n",
    "    for p in model.parameters(): p.realize()\n",
    "\n",
    "  def test_conv(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyConvNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, steps=100)\n",
    "    assert evaluate(model, X_test, Y_test) > 0.94   # torch gets 0.9415 sometimes\n",
    "\n",
    "  def test_conv_with_bn(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyConvNet(has_batchnorm=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.003)\n",
    "    train(model, X_train, Y_train, optimizer, steps=200)\n",
    "    assert evaluate(model, X_test, Y_test) > 0.94\n",
    "\n",
    "  def test_sgd(self):\n",
    "    np.random.seed(1337)\n",
    "    model = TinyBobNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "    train(model, X_train, Y_train, optimizer, steps=600)\n",
    "    assert evaluate(model, X_test, Y_test) > 0.94   # CPU gets 0.9494 sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb09af4-d2d7-4b5a-9f0b-5b295c8f0f63",
   "metadata": {},
   "source": [
    "The TestMNIST class is a group of tests to validate the performance of the TinyBobNet and TinyConvNet models on the MNIST dataset. It uses the unittest framework in Python, which helps in automating and organizing tests for software. Each method in this class is an individual test case.\n",
    "\n",
    "test_sgd_onestep, test_sgd_threestep, and test_sgd_sixstep: These methods train the TinyBobNet model using the Stochastic Gradient Descent (SGD) optimizer, with one, three, and six training steps respectively. The steps parameter defines how many times the model's weights will be updated during the training process.\n",
    "\n",
    "test_adam_onestep and test_adam_threestep: These methods are similar to the previous ones but use the Adam optimizer instead of SGD. Adam is another type of optimization algorithm that can sometimes yield better results than SGD.\n",
    "\n",
    "test_conv_onestep: This method tests the TinyConvNet model using the SGD optimizer with just one training step.\n",
    "\n",
    "test_conv: This method tests the TinyConvNet model with the Adam optimizer, using a larger number of training steps (100). After training, it checks if the model's performance (accuracy) on the test data is above 0.94 (94%).\n",
    "\n",
    "test_conv_with_bn: This method is similar to test_conv, but it uses a version of TinyConvNet with batch normalization enabled. It also uses a different optimizer, AdamW, with a larger learning rate and more training steps.\n",
    "\n",
    "test_sgd: This method tests the TinyBobNet model with the SGD optimizer, using an even larger number of training steps (600). After training, it checks if the model's performance on the test data is above 0.94 (94%).\n",
    "\n",
    "The phrase np.random.seed(1337) is repeated in every test. It ensures that the random numbers generated during each test are always the same, which makes the tests repeatable: running the test multiple times will always produce the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a49ad7-116d-4697-9a0b-7c97c4ed48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b895c-a3a3-4909-956c-1bd19bb411b2",
   "metadata": {},
   "source": [
    "This line of code is often used when scripting in Python, and it serves a dual purpose: It can allow or prevent parts of code from being run when the modules are imported.\n",
    "\n",
    "The __name__ variable in Python represents the name of the current module (the file being run). When you run a Python script directly (like python my_script.py), the __name__ variable is set to __main__.\n",
    "\n",
    "On the other hand, if you import your Python file as a module into another script (like import my_script), the __name__ variable is then set to the name of that script/module (my_script).\n",
    "\n",
    "In this context, if __name__ == '__main__': is used to check if this script is being run directly. If it is, it runs the code within this if-statement, which is unittest.main().\n",
    "\n",
    "The unittest.main() function is used to run all the test methods from the test class TestMNIST. It goes through all methods that start with 'test' in the class and executes them one by one.\n",
    "\n",
    "So, in summary, if __name__ == '__main__': unittest.main() is saying: if this script is the main script being run, then execute all the tests defined in the script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
