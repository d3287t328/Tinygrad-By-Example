{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b684073-65b5-4789-968c-d21eb1c6b34d",
   "metadata": {},
   "source": [
    "This code demonstrates the basic use of the tinygrad library, which is a minimalist deep learning library in Python. The tinygrad library is a very compact and simplified version of deep learning, meant for learning and not for serious usage. Here's what each section of the code does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8a93c-90fc-46c8-a18a-c25ba3131329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.tensor import Tensor\n",
    "import tinygrad.nn.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45141368-1221-456a-ac91-8545e8773e93",
   "metadata": {},
   "source": [
    "These lines are simply importing the necessary modules and classes from the tinygrad library. The Tensor class is used to create and manipulate multi-dimensional arrays, similar to NumPy's ndarray but with the added benefit of being able to calculate gradients for backpropagation. The optim module contains various optimization algorithms, such as Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d65b8-a033-4546-94dd-557c6cda22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyBobNet:\n",
    "  def __init__(self):\n",
    "    self.l1 = Tensor.uniform(784, 128)\n",
    "    self.l2 = Tensor.uniform(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x.dot(self.l1).relu().dot(self.l2).log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b0b159-54c1-42e0-ad4f-9735cd9dbdd1",
   "metadata": {},
   "source": [
    "This defines a simple neural network called TinyBobNet. The network has two layers (l1 and l2), each of which is initialized with uniformly random weights. The sizes of these layers (784, 128, 10) suggest that the network is meant to be used for a problem with 784 input features, a hidden layer with 128 neurons, and 10 output classes.\n",
    "\n",
    "The forward method defines how an input tensor x is processed by the network. The input is first multiplied by the weights of the first layer (x.dot(self.l1)), then the rectified linear unit (ReLU) activation function is applied (.relu()), after which the result is multiplied by the weights of the second layer (dot(self.l2)). Finally, the logarithm of the softmax function is applied (.log_softmax()), which is a common choice for the output layer of a classification network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6bd29-ecfc-4886-a598-af08653fec30",
   "metadata": {},
   "source": [
    "model = TinyBobNet()\n",
    "optim = optim.SGD([model.l1, model.l2], lr=0.001)\n",
    "\n",
    "# ... complete data loader here. This comment indicates that the part of the code where the data is loaded and preprocessed would be placed here.\n",
    "\n",
    "out = model.forward(x)\n",
    "loss = out.mul(y).mean()\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047cb7c-fea2-403d-bbb5-8c075dc7eab0",
   "metadata": {},
   "source": [
    "At top is where an instance of TinyBobNet is created (the model), and the SGD optimizer is set up to optimize the weights of the model. The learning rate is set to 0.001.\n",
    "\n",
    "In this part of the code, the forward pass of the model is performed with some input x to produce output out. The loss is then calculated by element-wise multiplication of out and the true output y, followed by averaging over all elements of the resulting tensor. The gradients of the parameters with respect to the loss are then reset to zero with optim.zero_grad(), and the backpropagation is performed with loss.backward(). Finally, the optimizer's step method updates the parameters of the model using the calculated gradients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
